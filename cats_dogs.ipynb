{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cats_dogs.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pung7XDsuNEh",
        "colab_type": "code",
        "outputId": "d6782021-6bfc-4d1d-967d-76422e77de04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzLWSBan93AS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from os import listdir\n",
        "from PIL import ImageOps, Image\n",
        "from glob import glob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSCNIxHK5dpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fonction d'entrainement\n",
        "def training(model, train_dataloader, valid_dataloader=None, epoch=5, learning_rate=0.1, use_gpu=False):\n",
        "  \n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "  \n",
        "  train_accu = []\n",
        "  train_losses = []\n",
        "  \n",
        "  if valid_dataloader:\n",
        "    val_accu = []\n",
        "    val_loss = []\n",
        "    \n",
        "  for i in range(epoch):\n",
        "    model.train()\n",
        "    \n",
        "    print('Starting epoch number {} on {} ...'.format(i+1, epoch))\n",
        "    \n",
        "    true = []\n",
        "    pred = []\n",
        "    train_loss = []\n",
        "    len_train = len(train_dataloader)\n",
        "    for ind, batch in enumerate(train_dataloader):\n",
        "      print('\\rBatch : {}/{}'.format(ind+1, len_train), end='')\n",
        "      x_train, y_train = batch\n",
        "      \n",
        "      # Aller le GPU, fais ton taff\n",
        "      if use_gpu:\n",
        "        inputs = inputs.cuda()\n",
        "        targets = targets.cuda()\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "        \n",
        "      outputs = model(inputs)\n",
        "      \n",
        "      loss = criterion(outputs, targets)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      predictions = outputs.max(dim=1)[1]\n",
        "      \n",
        "      train_loss.append(loss.item())\n",
        "      true.extend(targets.data.cpu().numpy().tolist())\n",
        "      pred.extend(predictions.data.cpu().numpy().tolist())\n",
        "    \n",
        "    accu_score = accuracy_score(true, pred) * 100\n",
        "    loss_score = sum(train_loss) / len(train_loss)\n",
        "    \n",
        "    print('\\nTrain score : Accuracy = {:.2f} - Loss = {:.2f}'.format(accu_score, loss_score), end='')\n",
        "    \n",
        "    \n",
        "    train_accu.append(accuracy_score(true, pred) * 100)\n",
        "    train_losses.append(loss_score)\n",
        "      \n",
        "    if valid_dataloader:\n",
        "      vaccu, vloss = validating(model, valid_dataloader, use_gpu)\n",
        "      print(' | Validation score : Accuracy = {:.2f} - Loss = {:.2f}'.format(vaccu, vloss))\n",
        "      val_accu.append(vaccu)\n",
        "      val_loss.append(vloss)\n",
        "    else:\n",
        "      print()\n",
        "  \n",
        "  if valid_dataloader:\n",
        "    return train_accu, train_losses, val_accu, val_loss\n",
        "  else:\n",
        "    return train_accu, train_loss\n",
        "  \n",
        "  return None, None\n",
        "  \n",
        "\n",
        "# Fonction de validation\n",
        "def validating(model, dataloader, use_gpu=False):\n",
        "  true =[]\n",
        "  pred = []\n",
        "  val_loss = []\n",
        "\n",
        "  criterion = nn.NLLLoss()\n",
        "  model.eval()\n",
        "\n",
        "  for batch in dataloader:\n",
        "\n",
        "    x_valid, y_valid = batch\n",
        "\n",
        "    # On envoit les données au GPU pour le traitement\n",
        "    if use_gpu:\n",
        "      inputs = inputs.cuda()\n",
        "      targets = targets.cuda()\n",
        "    \n",
        "    outputs = model(inputs)\n",
        "\n",
        "    predictions = outputs.max(dim=1)[1]\n",
        "\n",
        "    val_loss.append(criterion(outputs, targets).item())\n",
        "    true.extend(targets.data.cpu().numpy().tolist())\n",
        "    pred.extend(predictions.data.cpu().numpy().tolist())\n",
        "  \n",
        "  accu_score = accuracy_score(true, pred) * 100\n",
        "  loss_score = sum(val_loss) / len(val_loss)\n",
        "  return accu_score, loss_score\n",
        "\n",
        "\n",
        "# Fonction de prédiction\n",
        "#def predict(model, sentence, vocab, max_len):\n",
        "#  tokens = word_tokenize(sentence)\n",
        "#  X_test_len = len(tokens)\n",
        "#  X_test = prepareData(tokens, vocab, max_len)\n",
        "#  X_test = torch.LongTensor(X_test).cuda()\n",
        "#  X_test_len = torch.FloatTensor(X_test_len).cuda()\n",
        "#  output = model(X_test, X_test_len)\n",
        "#  prediction = output.max(dim=1)[1]\n",
        "#  print(prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--yWNVSEXyUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cellule pour éviter les doublons dans les positifs et négatifs.\n",
        "\n",
        "files_pos = [f for f in listdir(\"/content/gdrive/My Drive/datas/pixabay/dogs/1\")]\n",
        "files_neg = [f for f in listdir(\"/content/gdrive/My Drive/datas/pixabay/dogs/0\")]\n",
        "\n",
        "cnt = 0\n",
        "for f in files_pos:\n",
        "  if f in files_neg:\n",
        "    print(\"{} in Negatives.\".format(f))\n",
        "    cnt += 1\n",
        "  else:\n",
        "    print(\"{} not in Negatives\".format(f))\n",
        "    \n",
        "print(cnt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emw2Tfr0VkDJ",
        "colab_type": "text"
      },
      "source": [
        "Dans ce cadre précis il faut ajouter du padding à chaque image. Pour se faire, nous devons rechercher la largeur max et la longueur max."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX7-e7L-6Lsi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AddPadding:\n",
        "  def __init__(self, maxHeight, maxWidth):\n",
        "    self.maxHeight = maxHeight\n",
        "    self.maxWidth = maxWidth\n",
        "\n",
        "  def __call__(self, img):\n",
        "    # Utilisation du code https://discuss.pytorch.org/t/add-padding-to-images/24309/3\n",
        "    delta_width = self.maxWidth - img.size[0]\n",
        "    delta_height = self.maxHeight - img.size[1]\n",
        "    pad_width = delta_width //2\n",
        "    pad_height = delta_height //2\n",
        "    padding = (pad_width,pad_height,delta_width-pad_width,delta_height-pad_height)\n",
        "    return ImageOps.expand(img, padding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9-t4BEMjcQA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1c94db03-f9b0-4c8c-dd4c-179704dcff65"
      },
      "source": [
        "# Importation des images pour avoir les dimensions max\n",
        "# TODO : Améliorer ce passage de code avec du multi-threading (ralentissement important à partir de 60%)\n",
        "img_path = \"/content/gdrive/My Drive/datas/pixabay/dogs/\"\n",
        "files = glob(img_path + \"1/*\")\n",
        "maxH = 0\n",
        "maxW = 0\n",
        "total = len(files)\n",
        "cnt = 0\n",
        "for f in files:\n",
        "  img = Image.open(f)\n",
        "  w = img.width\n",
        "  h = img.height\n",
        "  if w > maxW:\n",
        "    maxW = w\n",
        "  if h > maxH:\n",
        "    maxH = h\n",
        "  cnt += 1\n",
        "  print(\"\\rAvancement : {:.2f}%\".format(cnt/total*100), end='')\n",
        "\n",
        "files = None\n",
        "print(\"\\nMax height : {}\\nMax width : {}\".format(maxH, maxW))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Avancement : 100.00%Max height : 340\n",
            "Max width : 1093\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nktOEJIVV2AF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# On charge les dataloaders (en supprimant les images précédentes pour libérer l'espace)\n",
        "all_transforms = transforms.Compose(\n",
        "    [\n",
        "     AddPadding(maxH, maxW),\n",
        "     transforms.ToTensor()\n",
        "    ]\n",
        ")\n",
        "train_dataset = torchvision.datasets.ImageFolder(\n",
        "    root=img_path,\n",
        "    transform=all_transforms\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WsTrb1U9rgP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataloader(dataset):\n",
        "    train_loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=128,\n",
        "        num_workers=0,\n",
        "        shuffle=True\n",
        "    )\n",
        "    return train_loader\n",
        "\n",
        "data_loader = create_dataloader(train_dataset)\n",
        "\n",
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "# La classification se fait sur 2 classes uniquement\n",
        "model.fc = nn.Linear(512, 2)\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVCvJlgqpdmF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training(model, data_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}