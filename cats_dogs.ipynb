{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cats_dogs.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pung7XDsuNEh",
        "colab_type": "code",
        "outputId": "680b64be-764b-419d-f554-daf152a174ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzLWSBan93AS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from os import listdir\n",
        "from PIL import ImageOps, Image\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split as splitTrainingData\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSCNIxHK5dpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fonction d'entrainement\n",
        "def training(model, train_dataloader, valid_dataloader=None, epoch=5, learning_rate=0.1, use_gpu=False):\n",
        "  \n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "  \n",
        "  train_accu = []\n",
        "  train_losses = []\n",
        "  \n",
        "  if valid_dataloader:\n",
        "    val_accu = []\n",
        "    val_loss = []\n",
        "    \n",
        "  for i in range(epoch):\n",
        "    model.train()\n",
        "    \n",
        "    print('Starting epoch number {} on {} ...'.format(i+1, epoch))\n",
        "    \n",
        "    true = []\n",
        "    pred = []\n",
        "    train_loss = []\n",
        "    len_train = len(train_dataloader)\n",
        "    for ind, batch in enumerate(train_dataloader):\n",
        "      print('\\rBatch : {}/{}'.format(ind+1, len_train), end='')\n",
        "      inputs, targets = batch\n",
        "      \n",
        "      # Aller le GPU, fais ton taff\n",
        "      if use_gpu:\n",
        "        inputs = inputs.cuda()\n",
        "        targets = targets.cuda()\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "        \n",
        "      outputs = model(inputs)\n",
        "      \n",
        "      loss = criterion(outputs, targets)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      predictions = outputs.max(dim=1)[1]\n",
        "      \n",
        "      train_loss.append(loss.item())\n",
        "      true.extend(targets.data.cpu().numpy().tolist())\n",
        "      pred.extend(predictions.data.cpu().numpy().tolist())\n",
        "    \n",
        "    accu_score = accuracy_score(true, pred) * 100\n",
        "    loss_score = sum(train_loss) / len(train_loss)\n",
        "    \n",
        "    print('\\nTrain score : Accuracy = {:.2f} - Loss = {:.2f}'.format(accu_score, loss_score), end='')\n",
        "    \n",
        "    \n",
        "    train_accu.append(accuracy_score(true, pred) * 100)\n",
        "    train_losses.append(loss_score)\n",
        "      \n",
        "    if valid_dataloader:\n",
        "      vaccu, vloss = validating(model, valid_dataloader, use_gpu)\n",
        "      print(' | Validation score : Accuracy = {:.2f} - Loss = {:.2f}'.format(vaccu, vloss))\n",
        "      val_accu.append(vaccu)\n",
        "      val_loss.append(vloss)\n",
        "    else:\n",
        "      print()\n",
        "  \n",
        "  if valid_dataloader:\n",
        "    return train_accu, train_losses, val_accu, val_loss\n",
        "  else:\n",
        "    return train_accu, train_loss\n",
        "  \n",
        "  return None, None\n",
        "  \n",
        "\n",
        "# Fonction de validation\n",
        "def validating(model, dataloader, use_gpu=False):\n",
        "  true =[]\n",
        "  pred = []\n",
        "  val_loss = []\n",
        "\n",
        "  criterion = nn.NLLLoss()\n",
        "  model.eval()\n",
        "\n",
        "  for batch in dataloader:\n",
        "\n",
        "    inputs, targets = batch\n",
        "\n",
        "    # On envoit les données au GPU pour le traitement\n",
        "    if use_gpu:\n",
        "      inputs = inputs.cuda()\n",
        "      targets = targets.cuda()\n",
        "    \n",
        "    outputs = model(inputs)\n",
        "\n",
        "    predictions = outputs.max(dim=1)[1]\n",
        "\n",
        "    val_loss.append(criterion(outputs, targets).item())\n",
        "    true.extend(targets.data.cpu().numpy().tolist())\n",
        "    pred.extend(predictions.data.cpu().numpy().tolist())\n",
        "  \n",
        "  accu_score = accuracy_score(true, pred) * 100\n",
        "  loss_score = sum(val_loss) / len(val_loss)\n",
        "  return accu_score, loss_score\n",
        "\n",
        "\n",
        "# Fonction de prédiction\n",
        "#def predict(model, sentence, vocab, max_len):\n",
        "#  tokens = word_tokenize(sentence)\n",
        "#  X_test_len = len(tokens)\n",
        "#  X_test = prepareData(tokens, vocab, max_len)\n",
        "#  X_test = torch.LongTensor(X_test).cuda()\n",
        "#  X_test_len = torch.FloatTensor(X_test_len).cuda()\n",
        "#  output = model(X_test, X_test_len)\n",
        "#  prediction = output.max(dim=1)[1]\n",
        "#  print(prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--yWNVSEXyUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%script false\n",
        "# Cellule pour éviter les doublons dans les positifs et négatifs.\n",
        "# Lancer la cellule 2 fois pour que le calcul se fasse\n",
        "\n",
        "files_pos = [f for f in listdir(\"/content/gdrive/My Drive/datas/pixabay/dogs/1\")]\n",
        "files_neg = [f for f in listdir(\"/content/gdrive/My Drive/datas/pixabay/dogs/0\")]\n",
        "\n",
        "cnt = 0\n",
        "for f in files_pos:\n",
        "  if f in files_neg:\n",
        "    cnt += 1\n",
        "    \n",
        "print(cnt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emw2Tfr0VkDJ",
        "colab_type": "text"
      },
      "source": [
        "Dans ce cadre précis il faut ajouter du padding à chaque image. Pour se faire, nous devons rechercher la largeur max et la longueur max."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX7-e7L-6Lsi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AddPadding:\n",
        "  def __init__(self, maxHeight, maxWidth):\n",
        "    self.maxHeight = maxHeight\n",
        "    self.maxWidth = maxWidth\n",
        "\n",
        "  def __call__(self, img):\n",
        "    # Utilisation du code https://discuss.pytorch.org/t/add-padding-to-images/24309/3\n",
        "    delta_width = self.maxWidth - img.size[0]\n",
        "    delta_height = self.maxHeight - img.size[1]\n",
        "    pad_width = delta_width //2\n",
        "    pad_height = delta_height //2\n",
        "    padding = (pad_width,pad_height,delta_width-pad_width,delta_height-pad_height)\n",
        "    return ImageOps.expand(img, padding)\n",
        "\n",
        "def create_dataloader(dataset):\n",
        "    train_loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=64,\n",
        "        num_workers=0,\n",
        "        shuffle=True\n",
        "    )\n",
        "    return train_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WsTrb1U9rgP",
        "colab_type": "code",
        "outputId": "b700e28a-287b-4cf5-8ac2-79af46d3e259",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# On charge les dataloaders (en supprimant les images précédentes pour libérer l'espace)\n",
        "img_path = \"/content/gdrive/My Drive/datas/pixabay/dogs/\"\n",
        "maxH = 340\n",
        "maxW = 1162\n",
        "all_transforms = transforms.Compose(\n",
        "    [\n",
        "     AddPadding(maxH, maxW),\n",
        "     transforms.ToTensor()\n",
        "    ]\n",
        ")\n",
        "train_dataset = torchvision.datasets.ImageFolder(\n",
        "    root=img_path,\n",
        "    transform=all_transforms\n",
        ")\n",
        "\n",
        "# Dataloader\n",
        "data_loader = create_dataloader(train_dataset)\n",
        "\n",
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "# La classification se fait sur 2 classes uniquement\n",
        "model.fc = nn.Linear(512, 2)\n",
        "print(model)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpuzP3ysla8g",
        "colab_type": "code",
        "outputId": "b6b7fd3d-0bdf-46ef-86ea-7bf98517cbc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.conv1"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVCvJlgqpdmF",
        "colab_type": "code",
        "outputId": "7833736e-fb1f-4b1d-bc3b-c76c7e48843c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "training(model.cuda(), data_loader, use_gpu=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch number 1 on 5 ...\n",
            "Batch : 323/323\n",
            "Train score : Accuracy = 69.61 - Loss = 0.86\n",
            "Starting epoch number 2 on 5 ...\n",
            "Batch : 323/323\n",
            "Train score : Accuracy = 72.61 - Loss = 0.60\n",
            "Starting epoch number 3 on 5 ...\n",
            "Batch : 323/323\n",
            "Train score : Accuracy = 72.64 - Loss = 0.59\n",
            "Starting epoch number 4 on 5 ...\n",
            "Batch : 323/323\n",
            "Train score : Accuracy = 72.78 - Loss = 0.59\n",
            "Starting epoch number 5 on 5 ...\n",
            "Batch : 323/323\n",
            "Train score : Accuracy = 72.73 - Loss = 0.58\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([69.61406227301342,\n",
              "  72.6066534308266,\n",
              "  72.63570771391215,\n",
              "  72.77613674882572,\n",
              "  72.72771294368312],\n",
              " [0.5855599641799927,\n",
              "  0.5272305011749268,\n",
              "  0.6003022193908691,\n",
              "  0.5769608616828918,\n",
              "  0.5191434025764465,\n",
              "  0.5364089608192444,\n",
              "  0.6776167750358582,\n",
              "  0.5689002275466919,\n",
              "  0.6323218941688538,\n",
              "  0.4941909611225128,\n",
              "  0.6641713380813599,\n",
              "  0.554228663444519,\n",
              "  0.6021980047225952,\n",
              "  0.6427145600318909,\n",
              "  0.5987027883529663,\n",
              "  0.599273145198822,\n",
              "  0.5930231809616089,\n",
              "  0.597343921661377,\n",
              "  0.5315658450126648,\n",
              "  0.5899679064750671,\n",
              "  0.6405826210975647,\n",
              "  0.5401877760887146,\n",
              "  0.5037523508071899,\n",
              "  0.5743625164031982,\n",
              "  0.5797250270843506,\n",
              "  0.5852159261703491,\n",
              "  0.5745620131492615,\n",
              "  0.531157910823822,\n",
              "  0.49666881561279297,\n",
              "  0.5582928657531738,\n",
              "  0.6647825837135315,\n",
              "  0.5930289626121521,\n",
              "  0.5623475909233093,\n",
              "  0.6347811222076416,\n",
              "  0.5677080154418945,\n",
              "  0.6170021295547485,\n",
              "  0.6037501692771912,\n",
              "  0.5434162020683289,\n",
              "  0.5515047907829285,\n",
              "  0.6771818995475769,\n",
              "  0.6517022252082825,\n",
              "  0.621145486831665,\n",
              "  0.617892324924469,\n",
              "  0.5784879326820374,\n",
              "  0.5764786005020142,\n",
              "  0.4960702359676361,\n",
              "  0.6925747394561768,\n",
              "  0.5958772897720337,\n",
              "  0.5126057267189026,\n",
              "  0.599035918712616,\n",
              "  0.4667293429374695,\n",
              "  0.6146461963653564,\n",
              "  0.7291815280914307,\n",
              "  0.6268320083618164,\n",
              "  0.631711483001709,\n",
              "  0.6657024025917053,\n",
              "  0.6118665337562561,\n",
              "  0.5998461246490479,\n",
              "  0.54509437084198,\n",
              "  0.6734355092048645,\n",
              "  0.591081440448761,\n",
              "  0.4986375868320465,\n",
              "  0.7084846496582031,\n",
              "  0.6404262781143188,\n",
              "  0.5358503460884094,\n",
              "  0.7115803956985474,\n",
              "  0.5784315466880798,\n",
              "  0.5005824565887451,\n",
              "  0.5895702242851257,\n",
              "  0.6386246085166931,\n",
              "  0.5618439316749573,\n",
              "  0.5934092402458191,\n",
              "  0.5481016039848328,\n",
              "  0.7163703441619873,\n",
              "  0.6002537608146667,\n",
              "  0.703687846660614,\n",
              "  0.593559205532074,\n",
              "  0.6835662126541138,\n",
              "  0.5732875466346741,\n",
              "  0.6045358777046204,\n",
              "  0.5550923943519592,\n",
              "  0.6263792514801025,\n",
              "  0.5741326212882996,\n",
              "  0.4283527731895447,\n",
              "  0.6387297511100769,\n",
              "  0.5011961460113525,\n",
              "  0.5956934094429016,\n",
              "  0.5748679637908936,\n",
              "  0.5554735064506531,\n",
              "  0.5837386846542358,\n",
              "  0.5546247959136963,\n",
              "  0.5273798108100891,\n",
              "  0.5372105836868286,\n",
              "  0.5784059762954712,\n",
              "  0.6843572854995728,\n",
              "  0.6589297652244568,\n",
              "  0.5444024801254272,\n",
              "  0.5641129612922668,\n",
              "  0.5540493726730347,\n",
              "  0.6757304072380066,\n",
              "  0.6228059530258179,\n",
              "  0.5406712889671326,\n",
              "  0.5283975601196289,\n",
              "  0.5673729777336121,\n",
              "  0.5748849511146545,\n",
              "  0.5652357339859009,\n",
              "  0.6123528480529785,\n",
              "  0.5535055994987488,\n",
              "  0.5662859678268433,\n",
              "  0.6160340309143066,\n",
              "  0.5819496512413025,\n",
              "  0.6414410471916199,\n",
              "  0.6863269805908203,\n",
              "  0.6078268885612488,\n",
              "  0.7226967215538025,\n",
              "  0.5808871984481812,\n",
              "  0.46087130904197693,\n",
              "  0.46757757663726807,\n",
              "  0.7028177976608276,\n",
              "  0.6317617297172546,\n",
              "  0.5516648292541504,\n",
              "  0.5515174269676208,\n",
              "  0.561305046081543,\n",
              "  0.6169965267181396,\n",
              "  0.5988124012947083,\n",
              "  0.5604445338249207,\n",
              "  0.5421395897865295,\n",
              "  0.47061607241630554,\n",
              "  0.5931797623634338,\n",
              "  0.5574420094490051,\n",
              "  0.5268564820289612,\n",
              "  0.6318942904472351,\n",
              "  0.6299989223480225,\n",
              "  0.5103077292442322,\n",
              "  0.5757879018783569,\n",
              "  0.5747613906860352,\n",
              "  0.6238314509391785,\n",
              "  0.548137366771698,\n",
              "  0.6738528609275818,\n",
              "  0.5837928056716919,\n",
              "  0.5972775220870972,\n",
              "  0.5522827506065369,\n",
              "  0.5867880582809448,\n",
              "  0.622645914554596,\n",
              "  0.5655761361122131,\n",
              "  0.6723181009292603,\n",
              "  0.6349841952323914,\n",
              "  0.6496071815490723,\n",
              "  0.5535748600959778,\n",
              "  0.5064436793327332,\n",
              "  0.6490899324417114,\n",
              "  0.5707743167877197,\n",
              "  0.6265615224838257,\n",
              "  0.6417964696884155,\n",
              "  0.5357168912887573,\n",
              "  0.6291181445121765,\n",
              "  0.5271453857421875,\n",
              "  0.7137740254402161,\n",
              "  0.6146828532218933,\n",
              "  0.5853530764579773,\n",
              "  0.5664955973625183,\n",
              "  0.5867516398429871,\n",
              "  0.6304370760917664,\n",
              "  0.543679416179657,\n",
              "  0.5620294213294983,\n",
              "  0.5593094825744629,\n",
              "  0.6015399694442749,\n",
              "  0.5789475440979004,\n",
              "  0.5254122614860535,\n",
              "  0.5301557779312134,\n",
              "  0.6062634587287903,\n",
              "  0.602803111076355,\n",
              "  0.5265976786613464,\n",
              "  0.6169178485870361,\n",
              "  0.5225545763969421,\n",
              "  0.5919709801673889,\n",
              "  0.6466317772865295,\n",
              "  0.6214689016342163,\n",
              "  0.5685983896255493,\n",
              "  0.6201647520065308,\n",
              "  0.6179390549659729,\n",
              "  0.6404603719711304,\n",
              "  0.5879637002944946,\n",
              "  0.6384330987930298,\n",
              "  0.5966405868530273,\n",
              "  0.6007484197616577,\n",
              "  0.543133556842804,\n",
              "  0.554496169090271,\n",
              "  0.5658667087554932,\n",
              "  0.46298861503601074,\n",
              "  0.6097816228866577,\n",
              "  0.5951510667800903,\n",
              "  0.5279200673103333,\n",
              "  0.7995900511741638,\n",
              "  0.6665458083152771,\n",
              "  0.5435733199119568,\n",
              "  0.5578961968421936,\n",
              "  0.5805185437202454,\n",
              "  0.5553585886955261,\n",
              "  0.6267575025558472,\n",
              "  0.5770375728607178,\n",
              "  0.6175144910812378,\n",
              "  0.5198710560798645,\n",
              "  0.44758379459381104,\n",
              "  0.5655811429023743,\n",
              "  0.545667290687561,\n",
              "  0.6241735219955444,\n",
              "  0.5892868638038635,\n",
              "  0.5376553535461426,\n",
              "  0.5746678113937378,\n",
              "  0.5704837441444397,\n",
              "  0.6312151551246643,\n",
              "  0.5710508227348328,\n",
              "  0.5938621759414673,\n",
              "  0.49870771169662476,\n",
              "  0.5739757418632507,\n",
              "  0.6617298722267151,\n",
              "  0.6524776816368103,\n",
              "  0.5480401515960693,\n",
              "  0.6477070450782776,\n",
              "  0.5889053344726562,\n",
              "  0.5936933755874634,\n",
              "  0.5619673132896423,\n",
              "  0.6227929592132568,\n",
              "  0.5123980045318604,\n",
              "  0.6968386173248291,\n",
              "  0.6030513644218445,\n",
              "  0.5842559933662415,\n",
              "  0.45449793338775635,\n",
              "  0.458658367395401,\n",
              "  0.6581653356552124,\n",
              "  0.6136583089828491,\n",
              "  0.6170188784599304,\n",
              "  0.49274298548698425,\n",
              "  0.4743114411830902,\n",
              "  0.5562328100204468,\n",
              "  0.5455601811408997,\n",
              "  0.5969792008399963,\n",
              "  0.5595817565917969,\n",
              "  0.5545374155044556,\n",
              "  0.5537834167480469,\n",
              "  0.5138828158378601,\n",
              "  0.43577519059181213,\n",
              "  0.6984610557556152,\n",
              "  0.5956315398216248,\n",
              "  0.5896593928337097,\n",
              "  0.5418592691421509,\n",
              "  0.45073190331459045,\n",
              "  0.5862470269203186,\n",
              "  0.49814867973327637,\n",
              "  0.5416014194488525,\n",
              "  0.6132106184959412,\n",
              "  0.5651629567146301,\n",
              "  0.5323310494422913,\n",
              "  0.5233106017112732,\n",
              "  0.4623606204986572,\n",
              "  0.4702063798904419,\n",
              "  0.5872271060943604,\n",
              "  0.5306465029716492,\n",
              "  0.5526338815689087,\n",
              "  0.5817717909812927,\n",
              "  0.6027155518531799,\n",
              "  0.6191050410270691,\n",
              "  0.588001549243927,\n",
              "  0.7661654353141785,\n",
              "  0.6439474821090698,\n",
              "  0.5739405155181885,\n",
              "  0.5330576300621033,\n",
              "  0.5270994901657104,\n",
              "  0.6614971160888672,\n",
              "  0.5988242626190186,\n",
              "  0.6192803382873535,\n",
              "  0.5956443548202515,\n",
              "  0.6404287219047546,\n",
              "  0.5743975639343262,\n",
              "  0.5910840034484863,\n",
              "  0.5279444456100464,\n",
              "  0.49974969029426575,\n",
              "  0.5376049280166626,\n",
              "  0.5315850973129272,\n",
              "  0.5093641877174377,\n",
              "  0.6030098795890808,\n",
              "  0.5592671036720276,\n",
              "  0.7038140296936035,\n",
              "  0.633588969707489,\n",
              "  0.48384416103363037,\n",
              "  0.678758978843689,\n",
              "  0.5665701627731323,\n",
              "  0.5342603921890259,\n",
              "  0.5616410374641418,\n",
              "  0.43817681074142456,\n",
              "  0.6379294991493225,\n",
              "  0.7588316202163696,\n",
              "  0.680201530456543,\n",
              "  0.6612694263458252,\n",
              "  0.5817378759384155,\n",
              "  0.6909144520759583,\n",
              "  0.6331572532653809,\n",
              "  0.6114964485168457,\n",
              "  0.5376158952713013,\n",
              "  0.5168152451515198,\n",
              "  0.671249270439148,\n",
              "  0.6450566053390503,\n",
              "  0.5996862649917603,\n",
              "  0.5619350671768188,\n",
              "  0.6669638752937317,\n",
              "  0.5629236102104187,\n",
              "  0.5194411277770996,\n",
              "  0.6036037802696228,\n",
              "  0.4440421462059021,\n",
              "  0.5495972633361816,\n",
              "  0.5233373641967773,\n",
              "  0.6353868842124939,\n",
              "  0.5788686275482178,\n",
              "  0.5463544130325317,\n",
              "  0.5184109210968018,\n",
              "  0.50287264585495,\n",
              "  0.6585462689399719,\n",
              "  0.5393705368041992,\n",
              "  0.626043975353241,\n",
              "  0.5589146018028259,\n",
              "  0.5265228748321533,\n",
              "  0.5838568210601807])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxzDyhN8icQO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "font_config = {\n",
        "    'family': 'serif',\n",
        "    'weight': 'normal',\n",
        "    'color': 'white',\n",
        "    'size': 20\n",
        "}\n",
        "\n",
        "axe_font = {\n",
        "    'family': 'serif',\n",
        "    'weight': 'normal',\n",
        "    'color': 'white',\n",
        "    'size': 10\n",
        "}\n",
        "\n",
        "epoch = len(train_accu)\n",
        "epochs = [x for x in range(1, epoch + 1)]\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
        "\n",
        "axes[0].set_xlabel('Epochs', fontdict=font_config)\n",
        "axes[0].set_ylabel('Accuracy', fontdict=font_config)\n",
        "axes[0].tick_params(axis='both', labelcolor='white', color='white')\n",
        "axes[0].plot(epochs, train_accu, label='Train')\n",
        "axes[0].plot(epochs, val_accu, label='Validation')\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].set_xlabel('Epochs', fontdict=font_config)\n",
        "axes[1].set_ylabel('Loss', fontdict=font_config)\n",
        "axes[1].tick_params(axis='both', labelcolor='white', color='white')\n",
        "axes[1].plot(epochs, train_loss, label='Train')\n",
        "axes[1].plot(epochs, val_loss, label='Validation')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}